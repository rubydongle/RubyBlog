---
title: 页表
categories:
  - 技术文章
date: 2019-09-04 17:38:21
tags:
  - Linux
  - 内核
  - 内存
---

## 物理内存页的划分
因为MMU以页(page)为单位对物理内存进行处理，所以物理内存被划分为一个个物理页来进行管理。  
不同体系机构支持页大小不尽相同，有些体系结构甚至支持集中不同的页大小。大多数32位体系结构支持4KB的页，而64位体系结构一般会支持8KB的页。这意味着，在支持4KB页的1GB物理内存机器上，物理页被划分为161244个页。  
内核中用struct page表示系统中的物理页。  
include/linux/mm_types.h  
```
/*
 * Each physical page in the system has a struct page associated with
 * it to keep track of whatever it is we are using the page for at the
 * moment. Note that we have no way to track which tasks are using
 * a page, though if it is a pagecache page, rmap structures can tell us
 * who is mapping it.
 *
 * The objects in struct page are organized in double word blocks in
 * order to allows us to use atomic double word operations on portions
 * of struct page. That is currently only used by slub but the arrangement
 * allows the use of atomic double word operations on the flags/mapping
 * and lru list pointers also.
 */
struct page {
        /* First double word block */
        unsigned long flags;            /* Atomic flags, some possibly
                                         * updated asynchronously */
        union {
                struct address_space *mapping;  /* If low bit clear, points to
                                                 * inode address_space, or NULL.
                                                 * If page mapped as anonymous
                                                 * memory, low bit is set, and
                                                 * it points to anon_vma object:
                                                 * see PAGE_MAPPING_ANON below.
                                                 */
                void *s_mem;                    /* slab first object */
                atomic_t compound_mapcount;     /* first tail page */
                /* page_deferred_list().next     -- second tail page */
        };

        /* Second double word */
        union {
                pgoff_t index;          /* Our offset within mapping. */
                void *freelist;         /* sl[aou]b first free object */
                /* page_deferred_list().prev    -- second tail page */
        };

        union {
#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \
        defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)
                /* Used for cmpxchg_double in slub */
                unsigned long counters;
#else
                /*
                 * Keep _refcount separate from slub cmpxchg_double data.
                 * As the rest of the double word is protected by slab_lock
                 * but _refcount is not.
                 */

                unsigned counters;
#endif
                struct {

                        union {
                                /*
                                 * Count of ptes mapped in mms, to show when
                                 * page is mapped & limit reverse map searches.
                                 *
                                 * Extra information about page type may be
                                 * stored here for pages that are never mapped,
                                 * in which case the value MUST BE <= -2.
                                 * See page-flags.h for more details.
                                 */
                                atomic_t _mapcount;

                                unsigned int active;            /* SLAB */
                                struct {                        /* SLUB */
                                        unsigned inuse:16;
                                        unsigned objects:15;
                                        unsigned frozen:1;
                                };
                                int units;                      /* SLOB */
                        };
                        /*
                         * Usage count, *USE WRAPPER FUNCTION* when manual
                         * accounting. See page_ref.h
                         */
                        atomic_t _refcount;
                };
        };

        /*
         * Third double word block
         *
         * WARNING: bit 0 of the first word encode PageTail(). That means
         * the rest users of the storage space MUST NOT use the bit to
         * avoid collision and false-positive PageTail().
         */
        union {
                struct list_head lru;   /* Pageout list, eg. active_list
                                         * protected by zone_lru_lock !
                                         * Can be used as a generic list
                                         * by the page owner.
                                         */
                struct dev_pagemap *pgmap; /* ZONE_DEVICE pages are never on an
                                            * lru or handled by a slab
                                            * allocator, this points to the
                                            * hosting device page map.
                                            */
                struct {                /* slub per cpu partial pages */
                        struct page *next;      /* Next partial slab */
#ifdef CONFIG_64BIT
                        int pages;      /* Nr of partial slabs left */
                        int pobjects;   /* Approximate # of objects */
#else
                        short int pages;
                        short int pobjects;
#endif
                };

                struct rcu_head rcu_head;       /* Used by SLAB
                                                 * when destroying via RCU
                                                 */
                /* Tail pages of compound page */
                struct {
                        unsigned long compound_head; /* If bit zero is set */

                        /* First tail page only */
#ifdef CONFIG_64BIT
                        /*
                         * On 64 bit system we have enough space in struct page
                         * to encode compound_dtor and compound_order with
                         * unsigned int. It can help compiler generate better or
                         * smaller code on some archtectures.
                         */
                        unsigned int compound_dtor;
                        unsigned int compound_order;
#else
                        unsigned short int compound_dtor;
                        unsigned short int compound_order;
#endif
                };

#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && USE_SPLIT_PMD_PTLOCKS
                struct {
                        unsigned long __pad;    /* do not overlay pmd_huge_pte
                                                 * with compound_head to avoid
                                                 * possible bit 0 collision.
                                                 */
                        pgtable_t pmd_huge_pte; /* protected by page->ptl */
                };
#endif
        };

        /* Remainder is not double word aligned */
        union {
                unsigned long private;          /* Mapping-private opaque data:
                                                 * usually used for buffer_heads
                                                 * if PagePrivate set; used for
                                                 * swp_entry_t if PageSwapCache;
                                                 * indicates order in the buddy
                                                 * system if PG_buddy is set.
                                                 */
#if USE_SPLIT_PTE_PTLOCKS
#if ALLOC_SPLIT_PTLOCKS
                spinlock_t *ptl;
#else
                spinlock_t ptl;
#endif
#endif
                struct kmem_cache *slab_cache;  /* SL[AU]B: Pointer to slab */
        };

#ifdef CONFIG_MEMCG
        struct mem_cgroup *mem_cgroup;
#endif

        /*
         * On machines where all RAM is mapped into kernel address space,
         * we can simply calculate the virtual address. On machines with
         * highmem some memory is mapped into kernel virtual memory
         * dynamically, so we need a place to store that address.
         * Note that this field could be 16 bits on x86 ... ;)
         *
         * Architectures with slow multiplication can define
         * WANT_PAGE_VIRTUAL in asm/page.h
         */
#if defined(WANT_PAGE_VIRTUAL)
        void *virtual;                  /* Kernel virtual address (NULL if
                                           not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */

#ifdef CONFIG_KMEMCHECK
        /*
         * kmemcheck wants to track the status of each byte in a page; this
         * is a pointer to such a status block. NULL if not tracked.
         */
        void *shadow;
#endif

#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
        int _last_cpupid;
#endif
}
```
简化结构：  
```
struct page {
    unsigned long		flags;
    atomic_t			_count;
    atomic_t			_mapcount;
    unsigned long 		private;
    struct address_space	*mapping;
    pgoff_t			index;
    struct list_head		lru;
    void			*virtual;
}
```

## 2.页表
[https://blog.csdn.net/yrj/article/details/2508785](https://blog.csdn.net/yrj/article/details/2508785)
Linux内核软件架构习惯被分成硬件相关层和硬件无关层。对于页表管理，2.6.10以前（包括2.6.10）在硬件无关层使用了3级页表目录管理的方式，它不管底层硬件是否实现的也是3级的页表管理：  
- Page Global Directory (PGD)
- Page Middle Directory (PMD)
- Page Table (PTE)

从2.6.11开始，为了配合64位CPU的体系结构，硬件无关层则使用了4级页表目录管理的方式：  
- Page Global Directory (PGD)
- Page Upper Directory (PUD)
- Page Middle Directory (PMD)
- Page Table (PTE)

PGD每个条目中指向一个PUD，PUD的每个条目指向一个PMD，PMD的每个条目指向一个PTE，PTE的每个条目指向一个页面(Page)的物理首地址。因此一个线性地址被分为了5个部分，如下图：
![img](/images/mm/PGT.JPG)

PGD，PUD，PMD，PTE中到底有几个条目，不同的CPU体系结构有不同的定义。
虽然硬件无关层是这么设计的，但是底层硬件未必也是这样实现的。如x86体系结构，如果不使用PAE（Physical Address Extension)特性，则硬件底层实现的是2级的页表目录管理，事实上，只有PGD，PTE才是真正有意义的。

- 页目录（Page directory）
    每个进程所代表的上下文数据结构中都有一个指针（mm_struct->pgd），其指向这个进程所使用的PGD的一个页（page frame）。  
    这个页面中包含了一个类型为pgd_t的数组。pgd的载入到CPU的方式完全和体系结构相关。
    x86，进程的页表地址从mm_struct->pgd载入到CR3寄存器，载入页表地址的同时，会引起TLB（快表，是对页目录，页表缓存的缓冲区）也被强制刷新。事实上，这也是__flush_tlb()函数，实现的机制。

    PGD中的每个条目指向一个页（page frame）， 这个页是“由类型为pud_t的条目组成的PUD”。 
    PUD中的每个条目同样指向一个页，这个页是“由类型为pmd_t的条目组成的PMD”。  
    PMD的每个条目指向一个页，这个页是“由类型为pte_t的条目组成的PTE”。  
    PTE的每个条目就指向了真正的数据或指令所在的页面的首地址的了，这也不是100%的，如果所需要的页面被交换到磁盘空间去后，这个条目就包含的内容是在当page fault发生后，传入需要调用的 do_swap_page()函数，找到包含页面数据的交换空间。

    将线性地址转换成物理地址，需要将线性地址分成5个部分，其中4个的值是在各级页表中的索引或者也可以看成是偏移(OFFSET)，另外一个是数据在页中的偏移。
    为了分别析出这5个部分，各级页表和页中偏移都拥有特定的几个宏：SHIFT，SIZE和MASK。SHIFT宏表示各级页表或页中偏移所占用的bit数。

    MASK的值和线性地址做AND运算，获得一个各级的高位部分，一般用于页面，页表对齐。SIZE宏表示各级所能管理的内存空间的字节数。
    ![img](/images/mm/LinearAddressBitSizeMacros.JPG)
    MASK和SIZE都是有SHIFT计算得到，如x86体系结构是这样的：
    ```
    #define PAGE_SHIFT??? 12
    #define PAGE_SIZE??? (1UL << PAGE_SHIFT)
    #define PAGE_MASK?? (~ (PAGE_SIZE - 1))
    ```

    PAGE_SHIFT是线性地址中偏移(offset)的位的位数，x86系统是12位。page的字节数计算很简单：2PAGE_SHIFT （和1<<PAGE_SHIFT是同样的结果）。如果需要对一个地址做页边界的对齐，则使用PAGE_ALIGN()宏，这个宏将地址加上PAGE_SIZE-1再和PAGE_MASK做AND操作即可。事实上PAGE_ALIGN()宏是和下一个页的边界对齐的。
    PMD_SHIFT是线性地址中第三级页表的所占的位数，PMD_SIZE和PMD_MARK是由这个宏计算得到的。
    PUD_SHIFT是线性地址中第二级页表的所占的位数，PUD_SIZE和PUD_MARK是由这个宏计算得到的。
    PGD_SHIFT是线性地址中第一级页表的所占的位数，PGD_SIZE和PGD_MARK是由这个宏计算得到的。
    ![img](/images/mm/LinearAddressSizeAndMaskMacros.JPG)
    最后介绍4个重要的宏：PTRS_PER_PGD,PTRS_PER_PUD, PTRS_PER_PMD,PTRS_PER_PTE。它们用于确定每级页表有多少条目。
    不使能PAE特性的x86体系结构这几个宏定义如下：
    ```
    #define PTRS_PER_PGD? 1024
    #define PTRS_PER_PUD??? 1? //这种情况下PUD事实不起作用，为了代码的硬件无关性，设置为1。

    //在include/asm-generic/pgtable-nopud.h中定义
    #define PTRS_PER_PMD? 1? //这种情况下PMD事实不起作用，为了代码的硬件无关性，设置为1。

    //在include/asm-generic/pgtable-nopmd.h中定义

    #define PTRS_PER_PTE? 1024
    ```

- 页表条目(Page table entry)
    页表的每个条目都是一个声明为数据结构的对象：pgd_t,pud_t,pmd_t和pte_t分别对应PGD,PUD,PMD和PTE。
    虽然这些数据结构常常只有一个无符号整数，它们被定义成数据结构有2个原因：第一，类型保护，防止被不合适的方式使用。第二，容易扩展每个条目所占字节的数量，如x86使能PAE，则需要另外加入4位(原书是说4位,但是我觉得应该是错误的，应该是加入了4个字节），以使得能够访问多余4GB的物理内存。
    为了保持一些保护位，定义了pgprot_t数据结构，它保存相关的标志，通常会保持在页表条目的低位区域。

    为了类型的计算，在文件asm/page_32.h或者asm/page_64.h中定义了5个宏。传入上述的类型，返回相应的数据结构中的部分数值：pte_val(),pmd_val(),pud_val()和pgprot_val(). 相反的操作的计算的宏：__pte(),__pmd(),__pud(),__pgd()和__pgprot()。

    条目中的状态位，完全是和体系结构相关的。下面解释一下不使能PAE的x86体系结构下，各个状态位的含义。
    没有使能PAE的x86，pte_t数据结构中只有一个32位的整数。每个PTE中的类型为pte_t的指针指向一个页面的首地址，也就是说指向的地址总是页面对齐的。因此，在这个整数中PAGE_SHIFT指定数目的位数，也就是12位，是给页表条目中的状态位。列表如下：
    ![img](/images/mm/PTEProtectionAndStatusBits.JPG)
    比较费解的是_PAGE_PROTNONE这个状态位，x86的体系结构上并不存在这个状态位，LINUX内核借用了PAT位作为这个来使用。  
    这里还有一个问题如果有PSE位被设置，则PAT位的位置就会使用另外一个位置，幸运的是，LINUX内核不会在用户页面中使用PSE特性。  
    LINUX内核挪用这个位的目的是：确定一个虚拟内存的页面在物理内存中是存在的，但是用户空间的进程不能访问它，如同对一段内存区域调用mprotect() API函数并传入PROT_NONE标志一样。当一段内存区域被要求保护，_PAGE_PRESENT为被清除，_PAGE_PROTNONE位被置一。
    pte_present()宏会同时检测这2位的设置情况，让kernel能够自己知道对应的PTE是否可用：
    #define pte_present(x)? ((x).pte_low & (_PAGE_PRESENT | _PAGE_PROTNONE))

    如果正好是用户空间不能访问的页面，这就相当巧妙了，但是也相当的重要考量。因为硬件状态为_PAGE_PRESENT已经被清除，当试图访问这个页面的时候，会产生一个page fault的异常，LINUX内核强制的保护了页面访问，但是内核还是知道页面是存在的，如果需要交换到磁盘或者进程退出释放页面，能够做出正确的动作。


[ARMv8(aarch64)页表建立过程详细分析](https://my.oschina.net/victorlovecode/blog/344394)
通过配置CONFIG_ARM64_PTDUMP=y后我们可以通过/sys/kernel/debug/kernel_page_tables查看页表情况。  
[kernel_page_tables](/files/mm/kernel_page_tables.txt)
```
---[ Modules start ]---
---[ Modules end ]---
---[ vmalloc() Area ]---
0xffffff8008000000-0xffffff8008010000          64K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008011000-0xffffff8008012000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008013000-0xffffff8008014000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008015000-0xffffff8008016000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008017000-0xffffff800801a000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff800801b000-0xffffff800801e000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff800801f000-0xffffff8008022000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff8008023000-0xffffff8008026000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff8008027000-0xffffff800802a000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff800802b000-0xffffff800802e000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff800802f000-0xffffff8008032000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff8008033000-0xffffff8008036000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
0xffffff8008037000-0xffffff8008038000           4K PTE       ro NX SHD AF NG         UXN MEM/NORMAL
0xffffff8008039000-0xffffff8008079000         256K PTE       RW NX SHD AF NG         UXN MEM/NORMAL-NC
0xffffff800807a000-0xffffff800807b000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff800807e000-0xffffff800807f000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008080000-0xffffff8008086000          24K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008088000-0xffffff800808e000          24K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff800808f000-0xffffff8008090000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008091000-0xffffff8008092000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008093000-0xffffff8008094000           4K PTE       RW NX SHD AF NG         UXN DEVICE/nGnRE
0xffffff8008095000-0xffffff80080a6000          68K PTE       RW NX SHD AF NG         UXN MEM/NORMAL-NC
......
0xffffff800d3f9000-0xffffff800d3fc000          12K PTE       RW NX SHD AF NG         UXN MEM/NORMAL-NC
0xffffff800d400000-0xffffff800d600000           2M PMD       RW NX SHD AF NG     BLK UXN DEVICE/nGnRE
0xffffff800d601000-0xffffff800d60b000          40K PTE       RW NX SHD AF NG         UXN MEM/NORMAL-NC
......
0xffffffbebff30000-0xffffffbebfff0000         768K PTE       RW NX SHD AF NG         UXN MEM/NORMAL
---[ vmalloc() End ]---
---[ Fixmap start ]---
0xffffffbefe7fc000-0xffffffbefe7fd000           4K PTE       ro x  SHD AF            UXN MEM/NORMAL
0xffffffbefe7fd000-0xffffffbefe7fe000           4K PTE       ro NX SHD AF NG         UXN MEM/NORMAL
0xffffffbefe800000-0xffffffbefea00000           2M PMD       ro NX SHD AF NG     BLK UXN MEM/NORMAL
---[ Fixmap end ]---
---[ PCI I/O start ]---
---[ PCI I/O end ]---
---[ vmemmap start ]---
0xffffffbfed000000-0xffffffbff5000000         128M PMD       RW NX SHD AF NG     BLK UXN MEM/NORMAL
---[ vmemmap end ]---
---[ Linear Mapping ]---
0xfffffffb40000000-0xfffffffb40080000         512K PTE F     RW NX SHD AF            UXN MEM/NORMAL
......
```

## 3.区
区通过struct zone表示  
include/linux/mmzone.h  
```
struct zone {
        /* Read-mostly fields */

        /* zone watermarks, access with *_wmark_pages(zone) macros */
        unsigned long watermark[NR_WMARK];

        unsigned long nr_reserved_highatomic;

        /*
         * We don't know if the memory that we're going to allocate will be
         * freeable or/and it will be released eventually, so to avoid totally
         * wasting several GB of ram we must reserve some of the lower zone
         * memory (otherwise we risk to run OOM on the lower zones despite
         * there being tons of freeable ram on the higher zones).  This array is
         * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
         * changes.
         */
        long lowmem_reserve[MAX_NR_ZONES];

#ifdef CONFIG_NUMA
        int node;
#endif
        struct pglist_data      *zone_pgdat;
        struct per_cpu_pageset __percpu *pageset;

#ifdef CONFIG_CMA
        bool                    cma_alloc;
#endif

#ifndef CONFIG_SPARSEMEM
        /*
         * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
         * In SPARSEMEM, this map is stored in struct mem_section
         */
        unsigned long           *pageblock_flags;
#endif /* CONFIG_SPARSEMEM */

        /* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
        unsigned long           zone_start_pfn;

        /*
         * spanned_pages is the total pages spanned by the zone, including
         * holes, which is calculated as:
         *      spanned_pages = zone_end_pfn - zone_start_pfn;
         *
         * present_pages is physical pages existing within the zone, which
         * is calculated as:
         *      present_pages = spanned_pages - absent_pages(pages in holes);
         *
         * managed_pages is present pages managed by the buddy system, which
         * is calculated as (reserved_pages includes pages allocated by the
         * bootmem allocator):
         *      managed_pages = present_pages - reserved_pages;
         *
         * So present_pages may be used by memory hotplug or memory power
         * management logic to figure out unmanaged pages by checking
         * (present_pages - managed_pages). And managed_pages should be used
         * by page allocator and vm scanner to calculate all kinds of watermarks
         * and thresholds.
         *
         * Locking rules:
         *
         * zone_start_pfn and spanned_pages are protected by span_seqlock.
         * It is a seqlock because it has to be read outside of zone->lock,
         * and it is done in the main allocator path.  But, it is written
         * quite infrequently.
         *
         * The span_seq lock is declared along with zone->lock because it is
         * frequently read in proximity to zone->lock.  It's good to
         * give them a chance of being in the same cacheline.
         *
         * Write access to present_pages at runtime should be protected by
         * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
         * present_pages should get_online_mems() to get a stable value.
         *
         * Read access to managed_pages should be safe because it's unsigned
         * long. Write access to zone->managed_pages and totalram_pages are
         * protected by managed_page_count_lock at runtime. Idealy only
         * adjust_managed_page_count() should be used instead of directly
         * touching zone->managed_pages and totalram_pages.
         */
        unsigned long           managed_pages;
        unsigned long           spanned_pages;
        unsigned long           present_pages;

        const char              *name;

#ifdef CONFIG_RSC_MEM_DEFRAG
        unsigned long   nr_migrate_unmovable_isolate_sord_block;
        unsigned long   nr_migrate_unmovable_isolate_bord_block;
#endif

#ifdef CONFIG_MEMORY_ISOLATION
        /*
         * Number of isolated pageblock. It is used to solve incorrect
         * freepage counting problem due to racy retrieving migratetype
         * of pageblock. Protected by zone->lock.
         */
        unsigned long           nr_isolate_pageblock;
#endif

#ifdef CONFIG_MEMORY_HOTPLUG
        /* see spanned/present_pages for more description */
        seqlock_t               span_seqlock;
#endif

        int initialized;

        /* Write-intensive fields used from the page allocator */
        ZONE_PADDING(_pad1_)

        /* free areas of different sizes */
        struct free_area        free_area[MAX_ORDER];

        /* zone flags, see below */
        unsigned long           flags;

        /* Primarily protects free_area */
        spinlock_t              lock;

        /* Write-intensive fields used by compaction and vmstats. */
        ZONE_PADDING(_pad2_)

        /*
         * When free pages are below this point, additional steps are taken
         * when reading the number of free pages to avoid per-cpu counter
         * drift allowing watermarks to be breached
         */
        unsigned long percpu_drift_mark;

#if defined CONFIG_COMPACTION || defined CONFIG_CMA
        /* pfn where compaction free scanner should start */
        unsigned long           compact_cached_free_pfn;
        /* pfn where async and sync compaction migration scanner should start */
        unsigned long           compact_cached_migrate_pfn[2];
#endif

#ifdef CONFIG_COMPACTION
        /*
         * On compaction failure, 1<<compact_defer_shift compactions
         * are skipped before trying again. The number attempted since
         * last failure is tracked with compact_considered.
         */
        unsigned int            compact_considered;
        unsigned int            compact_defer_shift;
        int                     compact_order_failed;
#endif

#if defined CONFIG_COMPACTION || defined CONFIG_CMA
        /* Set to true when the PG_migrate_skip bits should be cleared */
        bool                    compact_blockskip_flush;
#endif

        bool                    contiguous;

        ZONE_PADDING(_pad3_)
        /* Zone statistics */
        atomic_long_t           vm_stat[NR_VM_ZONE_STAT_ITEMS];
} ____cacheline_internodealigned_in_smp;
```

